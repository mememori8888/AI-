<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AIチューニングクイズ</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f3f4f6;
            color: #333;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            margin: 0;
            padding: 2rem;
            box-sizing: border-box;
        }
        .quiz-container {
            width: 100%;
            max-width: 800px;
            background-color: #ffffff;
            border-radius: 1.5rem;
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.1);
            padding: 2rem;
            box-sizing: border-box;
        }
        .question-card {
            background-color: #f9fafb;
            border-radius: 1rem;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
        }
        .question-text {
            font-size: 1.25rem;
            font-weight: 600;
            color: #1f2937;
            margin-bottom: 1.5rem;
            line-height: 1.6;
        }
        .answer-button {
            width: 100%;
            padding: 1rem;
            border-radius: 0.75rem;
            background-color: #e5e7eb;
            color: #4b5563;
            text-align: left;
            font-weight: 500;
            transition: all 0.2s ease-in-out;
            margin-bottom: 0.75rem;
            border: 2px solid transparent;
        }
        .answer-button:hover:not(.correct):not(.incorrect) {
            background-color: #d1d5db;
        }
        .answer-button.correct {
            background-color: #dcfce7;
            border-color: #22c55e;
            color: #166534;
        }
        .answer-button.incorrect {
            background-color: #fee2e2;
            border-color: #ef4444;
            color: #b91c1c;
        }
        .explanation {
            background-color: #f0f4f8;
            border-left: 4px solid #3b82f6;
            padding: 1rem;
            border-radius: 0.5rem;
            margin-top: 1rem;
            color: #1e40af;
        }
        .explanation-text {
            font-size: 1rem;
            line-height: 1.5;
        }
        .next-button {
            background-color: #3b82f6;
            color: #ffffff;
            padding: 0.75rem 2rem;
            border-radius: 0.75rem;
            font-weight: 600;
            transition: background-color 0.2s ease-in-out;
        }
        .next-button:hover {
            background-color: #2563eb;
        }
        .quiz-footer {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-top: 2rem;
        }
        .progress-text {
            font-weight: 600;
            color: #4b5563;
        }
    </style>
</head>
<body class="bg-gray-100 flex items-center justify-center min-h-screen p-4">

<div id="quiz-app" class="quiz-container">
    <h1 class="text-3xl font-bold text-center text-gray-800 mb-6">AIチューニングクイズ</h1>
    <div id="quiz-content">
        <!-- クイズのコンテンツがここに動的に挿入されます -->
    </div>
</div>

<script>
const questions = [
    {
      "question": "AIモデルのチューニングの目的は何ですか？",
      "hint": "このプロセスは、モデルが与えられたデータに対してより良く機能するようにすることです。",
      "options": [
        { "text": "モデルの学習時間を短縮すること", "isCorrect": false, "rationale": "モデルの学習時間を短縮することは目的の一つですが、最も重要な目的はモデルの性能を向上させることです。" },
        { "text": "モデルの性能を向上させ、特定のタスクで最適な結果を得ること", "isCorrect": true, "rationale": "AIモデルのチューニングの主要な目的は、モデルの性能を最適化し、特定のタスクで最高の精度や結果を達成することです。" },
        { "text": "モデルのサイズを最小限に抑えること", "isCorrect": false, "rationale": "モデルのサイズを最小化することはモデルの軽量化に関係しますが、直接的なチューニングの目的ではありません。" },
        { "text": "モデルを解釈可能にすること", "isCorrect": false, "rationale": "モデルの解釈可能性は別の研究分野であり、チューニングの直接的な目的ではありません。" }
      ]
    },
    {
      "question": "ハイパーパラメータチューニングとは何ですか？",
      "hint": "これは、モデルのトレーニングプロセスそのものではなく、そのプロセスをガイドする設定に関するものです。",
      "options": [
        { "text": "データの前処理方法を最適化すること", "isCorrect": false, "rationale": "データの前処理はモデルの準備段階ですが、ハイパーパラメータチューニングとは異なります。" },
        { "text": "モデルが学習する内部パラメータ（重みやバイアス）を調整すること", "isCorrect": false, "rationale": "これはモデルのトレーニングプロセスであり、ハイパーパラメータチューニングではありません。トレーニング中にモデルが自動的に調整します。" },
        { "text": "学習プロセスを開始する前に設定される外部パラメータを最適化すること", "isCorrect": true, "rationale": "ハイパーパラメータはモデルの学習プロセスを制御する外部設定であり、学習開始前に手動で設定または最適化する必要があります。" },
        { "text": "モデルのアーキテクチャを変更すること", "isCorrect": false, "rationale": "モデルのアーキテクチャを変更することは、モデルの設計の一部であり、ハイパーパラメータチューニングとは異なります。" }
      ]
    },
    {
      "question": "過学習（Overfitting）とは何ですか？",
      "hint": "この現象は、モデルが特定のデータセットに『記憶』しすぎて、現実世界の問題を解決できなくなることを指します。",
      "options": [
        { "text": "モデルがトレーニングデータではうまく機能するが、新しいデータでは性能が低下すること", "isCorrect": true, "rationale": "過学習は、モデルがトレーニングデータ内のノイズや特定のパターンを過剰に学習した結果、未知のデータに対する汎化能力が失われる現象です。" },
        { "text": "モデルがトレーニングデータでも新しいデータでもうまく機能しないこと", "isCorrect": false, "rationale": "これは過小学習（Underfitting）として知られる現象です。" },
        { "text": "モデルがトレーニングデータと新しいデータの両方で同様に機能すること", "isCorrect": false, "rationale": "これはモデルの汎化能力が高く、適切に学習されている状態です。" },
        { "text": "モデルがトレーニングデータを非常に速く学習すること", "isCorrect": false, "rationale": "学習速度は過学習と直接的な関係はありません。" }
      ]
    },
    {
      "question": "正則化（Regularization）の主な目的は何ですか？",
      "hint": "これは、過度な特化を防ぎ、モデルをより柔軟にするための技術です。",
      "options": [
        { "text": "モデルの学習速度を上げること", "isCorrect": false, "rationale": "正則化は、学習速度を直接的に制御するものではありません。" },
        { "text": "過学習を減らし、モデルの汎化能力を向上させること", "isCorrect": true, "rationale": "正則化は、モデルの複雑さを制限し、重みパラメータを小さく保つことで、過学習を防ぎ、モデルがより広範なデータセットに適用できるようにします。" },
        { "text": "トレーニングデータセットのサイズを減らすこと", "isCorrect": false, "rationale": "データセットのサイズは正則化とは関係ありません。" },
        { "text": "モデルの精度を犠牲にしてでも、常に誤差を最小限に抑えること", "isCorrect": false, "rationale": "正則化は、テストデータでの精度を向上させるために、トレーニング精度をわずかに犠牲にすることがあります。" }
      ]
    },
    {
      "question": "ドロップアウト（Dropout）とは、ニューラルネットワークの過学習を防ぐためにどのような手法ですか？",
      "hint": "この手法は、ニューラルネットワークの各層が、他の特定のニューロンに依存しすぎないようにします。",
      "options": [
        { "text": "トレーニング中にランダムに特定の入力を削除する", "isCorrect": false, "rationale": "ドロップアウトは、入力データではなく、ニューラルネットワークの隠れ層のニューロンを無効化します。" },
        { "text": "トレーニング中にランダムにニューロンを一時的に無効化する", "isCorrect": true, "rationale": "ドロップアウトは、各トレーニングサイクルでニューラルネットワークのニューロンのサブセットをランダムに「ドロップアウト」または一時的に無効化することで、ニューロン間の過度な依存関係を防ぎます。" },
        { "text": "トレーニング中にランダムにモデルのパラメータをリセットする", "isCorrect": false, "rationale": "モデルのパラメータをリセットすることは、ドロップアウトとは異なる手法です。" },
        { "text": "トレーニング中にデータセットからランダムにデータを削除する", "isCorrect": false, "rationale": "これはデータ増強（Data Augmentation）とは異なります。" }
      ]
    },
    {
      "question": "学習率（Learning Rate）が小さすぎる場合、AIモデルのトレーニングにどのような影響がありますか？",
      "hint": "これは、モデルが解に向かって進む『歩幅』が小さすぎることに関係します。",
      "options": [
        { "text": "トレーニングプロセスが非常に速くなる", "isCorrect": false, "rationale": "学習率が小さいと、モデルの重みの更新が遅くなるため、学習プロセスが遅くなります。" },
        { "text": "モデルが局所最適解に陥る可能性が高くなる", "isCorrect": false, "rationale": "学習率が大きすぎると、最適解を飛び越えてしまう可能性があり、学習率が小さすぎると、学習が遅くなります。" },
        { "text": "トレーニングプロセスが遅くなり、最適解に到達するのに時間がかかる", "isCorrect": true, "rationale": "学習率が小さすぎると、モデルのパラメータ更新が非常に小さくなり、モデルの収束が遅くなり、最適解に到達するまでに多くの反復が必要になります。" },
        { "text": "モデルが収束せず、損失が振動する", "isCorrect": false, "rationale": "学習率が大きすぎる場合、この現象が発生する可能性が高いです。" }
      ]
    },
    {
      "question": "学習率が大きすぎる場合、AIモデルのトレーニングにどのような影響がありますか？",
      "hint": "これは、モデルが解に向かって進む『歩幅』が大きすぎることに関係します。",
      "options": [
        { "text": "トレーニングプロセスが遅くなる", "isCorrect": false, "rationale": "学習率が大きいと、モデルの重みの更新が大きくなるため、学習プロセスは速くなります。" },
        { "text": "モデルの重みが大きく変動し、損失が収束せずに発散する可能性がある", "isCorrect": true, "rationale": "学習率が大きすぎると、モデルが最適解を『飛び越えて』しまい、損失が収束せずに発散したり、不安定になったりする可能性があります。" },
        { "text": "モデルがすぐに収束し、最高の性能を達成する", "isCorrect": false, "rationale": "学習率が大きすぎると、最適な解決策に達する可能性が低くなります。" },
        { "text": "トレーニングプロセスが停止する", "isCorrect": false, "rationale": "トレーニングプロセスは停止せず、不安定な収束を続ける可能性があります。" }
      ]
    },
    {
      "question": "早期停止（Early Stopping）とは、モデルの過学習を防ぐためのどのような戦略ですか？",
      "hint": "この手法は、モデルが新しいデータでうまく機能しなくなる兆候を検知します。",
      "options": [
        { "text": "トレーニングが開始される前にモデルのトレーニングを停止すること", "isCorrect": false, "rationale": "早期停止は、トレーニングが開始される前には適用できません。" },
        { "text": "トレーニングの損失が増加し始めたときにトレーニングを停止すること", "isCorrect": false, "rationale": "トレーニング損失は通常、トレーニング中に減少します。" },
        { "text": "検証損失が一定期間改善しなくなったときにトレーニングを停止すること", "isCorrect": true, "rationale": "早期停止は、モデルの性能が検証データセットで向上しなくなった時点でトレーニングを停止し、過学習を防ぎます。" },
        { "text": "すべてのトレーニングデータを使い切ったときにトレーニングを停止すること", "isCorrect": false, "rationale": "これは標準的なトレーニングの終了条件ですが、早期停止の概念とは異なります。" }
      ]
    },
    {
      "question": "交差検証（Cross-Validation）の主な目的は何ですか？",
      "hint": "これは、特定のデータ分割に対するモデルの依存度を減らすための技術です。",
      "options": [
        { "text": "トレーニング時間を短縮すること", "isCorrect": false, "rationale": "交差検証は、複数のモデルをトレーニングするため、実際にはトレーニング時間を延長します。" },
        { "text": "モデルがトレーニングデータに過学習していないことを確認すること", "isCorrect": false, "rationale": "過学習の確認も行いますが、主な目的ではありません。" },
        { "text": "モデルの性能をよりロバストに評価すること", "isCorrect": true, "rationale": "交差検証は、データセットを複数のサブセットに分割し、各サブセットでモデルをトレーニングして評価することで、モデルの性能評価をより信頼性の高いものにします。" },
        { "text": "ハイパーパラメータの最適な組み合わせを見つけること", "isCorrect": false, "rationale": "交差検証は、ハイパーパラメータチューニングのプロセスでモデルを評価するために使用されますが、それ自体が最適な組み合わせを見つける手法ではありません。" }
      ]
    },
    {
      "question": "バッチサイズが大きすぎる場合、モデルのトレーニングにどのような影響がありますか？",
      "hint": "これは、一度に処理されるデータの量と、その結果として生じる勾配の安定性に関係します。",
      "options": [
        { "text": "モデルがより速く収束する", "isCorrect": false, "rationale": "バッチサイズが大きいと、各エポックの反復回数が減り、計算が速くなることがありますが、収束は必ずしも速くなるわけではありません。" },
        { "text": "勾配の更新がより安定するが、計算リソースが多く必要になる", "isCorrect": true, "rationale": "バッチサイズが大きいと、勾配の推定がより正確になるため、勾配の更新が安定しますが、大きなメモリと計算能力が必要になります。" },
        { "text": "勾配の更新が不安定になり、損失が発散する可能性がある", "isCorrect": false, "rationale": "バッチサイズが小さい場合、この現象が起こりやすいです。" },
        { "text": "モデルの精度が大幅に向上する", "isCorrect": false, "rationale": "バッチサイズが大きいと、汎化能力が低下し、最終的な精度が低くなる可能性があります。" }
      ]
    },
    {
      "question": "バッチサイズが小さすぎる場合、モデルのトレーニングにどのような影響がありますか？",
      "hint": "これは、一度に処理されるデータの量と、その結果として生じる勾配の安定性に関係します。",
      "options": [
        { "text": "モデルの勾配更新が不安定になり、学習プロセスが遅くなる", "isCorrect": true, "rationale": "バッチサイズが小さいと、勾配の推定がノイズの影響を受けやすくなり、更新が不安定になります。ただし、メモリ使用量は少なくなります。" },
        { "text": "モデルの勾配更新が安定し、収束が速くなる", "isCorrect": false, "rationale": "バッチサイズが小さいと、勾配はより不安定になります。" },
        { "text": "モデルの収束が速くなり、計算リソースを多く消費する", "isCorrect": false, "rationale": "バッチサイズが小さいと、各反復の計算は速いですが、全体的な収束は遅くなることがあります。" },
        { "text": "モデルが過学習するのを防ぐ", "isCorrect": false, "rationale": "バッチサイズが小さいことは、過学習を防ぐ直接的な方法ではありません。" }
      ]
    },
    {
      "question": "損失関数（Loss Function）の主な役割は何ですか？",
      "hint": "これは、モデルの予測が実際の値からどれだけ離れているかを測定するものです。",
      "options": [
        { "text": "モデルのトレーニング速度を制御すること", "isCorrect": false, "rationale": "学習率がトレーニング速度を制御します。" },
        { "text": "モデルの重みを初期化すること", "isCorrect": false, "rationale": "これは初期化の役割です。" },
        { "text": "モデルの予測と実際の値の間の誤差を測定すること", "isCorrect": true, "rationale": "損失関数は、モデルのパフォーマンスを評価し、勾配降下法がパラメータを調整するためのフィードバックを提供します。" },
        { "text": "モデルのアーキテクチャを決定すること", "isCorrect": false, "rationale": "モデルのアーキテクチャは、モデルの設計者によって決定されます。" }
      ]
    },
    {
      "question": "最適化アルゴリズム（Optimizer）の主な役割は何ですか？",
      "hint": "これは、モデルが損失を最小化するためにどのように学習するかを決定します。",
      "options": [
        { "text": "データのノイズを減らすこと", "isCorrect": false, "rationale": "データのノイズを減らすことは、データの前処理の一部です。" },
        { "text": "モデルがトレーニングデータに過学習するのを防ぐこと", "isCorrect": false, "rationale": "正則化がこの役割を果たします。" },
        { "text": "損失関数を最小化するためにモデルの重みを更新すること", "isCorrect": true, "rationale": "最適化アルゴリズムは、損失関数の出力に基づいてモデルの重みを調整し、モデルの性能を向上させます。" },
        { "text": "トレーニングデータを検証データに分割すること", "isCorrect": false, "rationale": "これはデータ分割の役割です。" }
      ]
    },
    {
      "question": "Adam（Adaptive Moment Estimation）オプティマイザの主な利点は何ですか？",
      "hint": "このオプティマイザは、学習率を自動的に調整します。",
      "options": [
        { "text": "手動で学習率を設定する必要がないこと", "isCorrect": false, "rationale": "Adamを使用する場合でも、学習率を手動で設定する必要がありますが、他のオプティマイザよりも安定しています。" },
        { "text": "各パラメータに個別の適応的な学習率を提供すること", "isCorrect": true, "rationale": "Adamは、各パラメータの勾配の1次および2次モーメントを推定することで、トレーニング中に各パラメータに異なる学習率を適応させることができます。" },
        { "text": "学習速度を大幅に低下させること", "isCorrect": false, "rationale": "Adamは通常、トレーニングを加速します。" },
        { "text": "局所最適解に陥らないこと", "isCorrect": false, "rationale": "Adamは局所最適解に陥る可能性があります。" }
      ]
    },
    {
      "question": "モデルの性能を評価するために使用される最も一般的なメトリックはどれですか？",
      "hint": "これは、正しく予測されたサンプルの割合を測定します。",
      "options": [
        { "text": "精度（Accuracy）", "isCorrect": true, "rationale": "精度は、正しく分類されたサンプルの数を全体のサンプル数で割ったもので、分類タスクで最も一般的なメトリックです。" },
        { "text": "再現率（Recall）", "isCorrect": false, "rationale": "再現率は、すべての正のサンプルの中で、モデルが正しく予測した割合を測定します。" },
        { "text": "適合率（Precision）", "isCorrect": false, "rationale": "適合率は、モデルが正と予測したサンプルの中で、実際に正であった割合を測定します。" },
        { "text": "F1スコア", "isCorrect": false, "rationale": "F1スコアは、適合率と再現率の調和平均です。" }
      ]
    },
    {
      "question": "バッチ正規化（Batch Normalization）の目的は何ですか？",
      "hint": "これは、モデルの学習を安定させ、高速化するための手法です。",
      "options": [
        { "text": "過学習を防ぐこと", "isCorrect": false, "rationale": "過学習を防ぐ主な目的は、正則化やドロップアウトです。" },
        { "text": "トレーニングデータのサイズを増やすこと", "isCorrect": false, "rationale": "これはデータ増強（Data Augmentation）の役割です。" },
        { "text": "トレーニング中に各層の入力の分布を正規化し、学習を安定させること", "isCorrect": true, "rationale": "バッチ正規化は、各ミニバッチの入力の平均と分散を正規化することで、勾配の不安定性を減らし、学習を安定させ、高速化します。" },
        { "text": "モデルの重みをゼロに初期化すること", "isCorrect": false, "rationale": "これは初期化の役割です。" }
      ]
    },
    {
      "question": "モデルをゼロからトレーニングするのではなく、事前にトレーニングされたモデルを使用する手法は何と呼ばれますか？",
      "hint": "この手法は、既に学習した知識を新しいタスクに『転用』するものです。",
      "options": [
        { "text": "転移学習（Transfer Learning）", "isCorrect": true, "rationale": "転移学習は、あるタスクで学習されたモデルを、別の関連タスクの出発点として使用する手法です。" },
        { "text": "ファインチューニング（Fine-tuning）", "isCorrect": false, "rationale": "ファインチューニングは、転移学習の一部であり、事前学習済みモデルのパラメータを微調整するプロセスです。" },
        { "text": "モデルの量子化（Quantization）", "isCorrect": false, "rationale": "モデルの量子化は、モデルのサイズを減らし、高速化するための手法です。" },
        { "text": "モデルの蒸留（Distillation）", "isCorrect": false, "rationale": "モデルの蒸留は、より大きなモデルの知識を小さなモデルに転送する手法です。" }
      ]
    },
    {
      "question": "ファインチューニングとは、具体的にどのようなプロセスですか？",
      "hint": "これは、事前学習済みモデルを特定のタスクに合わせて調整することです。",
      "options": [
        { "text": "モデルのすべての層をゼロから再学習すること", "isCorrect": false, "rationale": "これはスクラッチからのトレーニングです。" },
        { "text": "事前学習済みモデルの最後の数層のみを新しいデータで再学習すること", "isCorrect": false, "rationale": "ファインチューニングでは、通常、モデルのすべての層を新しいデータで再学習しますが、学習率を低く設定します。" },
        { "text": "事前学習済みモデルのパラメータを、特定のタスクのデータで微調整すること", "isCorrect": true, "rationale": "ファインチューニングは、事前学習済みモデルの重みを新しいタスクのデータで微調整することで、モデルを特定のタスクに適応させます。" },
        { "text": "モデルのアーキテクチャ全体を再設計すること", "isCorrect": false, "rationale": "これはモデルの再設計であり、ファインチューニングではありません。" }
      ]
    },
    {
      "question": "学習率スケジューラ（Learning Rate Scheduler）の目的は何ですか？",
      "hint": "これは、トレーニング中に学習率を動的に変更するものです。",
      "options": [
        { "text": "モデルの重みを初期化すること", "isCorrect": false, "rationale": "学習率スケジューラは、学習率を初期化するものではありません。" },
        { "text": "モデルが過学習するのを防ぐこと", "isCorrect": false, "rationale": "学習率スケジューラは、過学習を防ぐのを助けることができますが、主な目的は、トレーニングの効率を向上させることです。" },
        { "text": "トレーニング中に学習率を自動的に調整し、収束を改善すること", "isCorrect": true, "rationale": "学習率スケジューラは、トレーニングの進行に応じて学習率を段階的に減少させることで、モデルが最適解に収束するのを助けます。" },
        { "text": "トレーニングデータのサイズを増やすこと", "isCorrect": false, "rationale": "これはデータ増強（Data Augmentation）の役割です。" }
      ]
    },
    {
      "question": "勾配クリッピング（Gradient Clipping）の目的は何ですか？",
      "hint": "これは、勾配の『爆発』を防ぐための技術です。",
      "options": [
        { "text": "勾配をゼロに設定すること", "isCorrect": false, "rationale": "勾配をゼロに設定することは、学習を停止させます。" },
        { "text": "勾配の大きさが特定の閾値を超えないようにすること", "isCorrect": true, "rationale": "勾配クリッピングは、勾配の大きさが閾値を超えた場合に、その勾配をスケーリングすることで、勾配の爆発を防ぎ、トレーニングの不安定性を減らします。" },
        { "text": "勾配の方向を変更すること", "isCorrect": false, "rationale": "勾配クリッピングは、勾配の方向を変更するものではありません。" },
        { "text": "勾配をランダムに初期化すること", "isCorrect": false, "rationale": "勾配をランダムに初期化することは、学習を妨げます。" }
      ]
    },
    {
      "question": "L1正則化の別名は何ですか？",
      "hint": "これは、正則化項が重みの絶対値の和であることに由来します。",
      "options": [
        { "text": "リッジ正則化（Ridge Regression）", "isCorrect": false, "rationale": "リッジ正則化はL2正則化の別名です。" },
        { "text": "ラッソ（Lasso）", "isCorrect": true, "rationale": "L1正則化は、Lasso（Least Absolute Shrinkage and Selection Operator）としても知られています。" },
        { "text": "エラスティックネット（Elastic Net）", "isCorrect": false, "rationale": "エラスティックネットは、L1正則化とL2正則化の両方を組み合わせたものです。" },
        { "text": "ドロップアウト", "isCorrect": false, "rationale": "ドロップアウトは、正則化の一種ですが、L1正則化とは異なります。" }
      ]
    },
    {
      "question": "L2正則化の別名は何ですか？",
      "hint": "これは、正則化項が重みの二乗和であることに由来します。",
      "options": [
        { "text": "ラッソ（Lasso）", "isCorrect": false, "rationale": "ラッソはL1正則化の別名です。" },
        { "text": "リッジ正則化（Ridge Regression）", "isCorrect": true, "rationale": "L2正則化は、リッジ正則化（Ridge Regression）としても知られています。" },
        { "text": "エラスティックネット（Elastic Net）", "isCorrect": false, "rationale": "エラスティックネットは、L1正則化とL2正則化の両方を組み合わせたものです。" },
        { "text": "ドロップアウト", "isCorrect": false, "rationale": "ドロップアウトは、正則化の一種ですが、L2正則化とは異なります。" }
      ]
    },
    {
      "question": "ハイパーパラメータチューニングの手法で、指定された範囲内でランダムにパラメータを試す方法は？",
      "hint": "これは、グリッドサーチよりも効率が良い場合があります。",
      "options": [
        { "text": "グリッドサーチ（Grid Search）", "isCorrect": false, "rationale": "グリッドサーチは、事前に定義された値のセットからパラメータのすべての組み合わせを試します。" },
        { "text": "ランダムサーチ（Random Search）", "isCorrect": true, "rationale": "ランダムサーチは、パラメータの指定された範囲からランダムに値を選び、試します。これにより、グリッドサーチよりも効率的に最適な組み合わせを見つけることができます。" },
        { "text": "ベイズ最適化（Bayesian Optimization）", "isCorrect": false, "rationale": "ベイズ最適化は、過去の結果に基づいて次の試行点をインテリジェントに選択する、より高度な手法です。" },
        { "text": "勾配降下法（Gradient Descent）", "isCorrect": false, "rationale": "勾配降下法は、モデルのパラメータを最適化するための手法であり、ハイパーパラメータチューニングではありません。" }
      ]
    },
    {
      "question": "ハイパーパラメータチューニングで、過去の結果に基づいて次の試行点をインテリジェントに選択する手法は？",
      "hint": "これは、『試行錯誤』を減らすための、より高度な方法です。",
      "options": [
        { "text": "グリッドサーチ（Grid Search）", "isCorrect": false, "rationale": "グリッドサーチは、事前に定義された値のセットからパラメータのすべての組み合わせを試します。" },
        { "text": "ランダムサーチ（Random Search）", "isCorrect": false, "rationale": "ランダムサーチは、パラメータの指定された範囲からランダムに値を選び、試します。" },
        { "text": "ベイズ最適化（Bayesian Optimization）", "isCorrect": true, "rationale": "ベイズ最適化は、過去の結果を利用して次の試行点を予測し、より効率的に最適な組み合わせを見つけます。" },
        { "text": "勾配降下法（Gradient Descent）", "isCorrect": false, "rationale": "勾配降下法は、モデルのパラメータを最適化するための手法であり、ハイパーパラメータチューニングではありません。" }
      ]
    },
    {
      "question": "トレーニングデータが不均衡な場合、精度（Accuracy）がモデルの性能を評価するのに不適切な理由は何ですか？",
      "hint": "例えば、99%が負のクラスであるデータセットを考えてみましょう。",
      "options": [
        { "text": "計算が複雑すぎるため", "isCorrect": false, "rationale": "精度の計算はシンプルです。" },
        { "text": "モデルが常に多数派のクラスを予測するだけで高い精度を達成できるため", "isCorrect": true, "rationale": "データが不均衡な場合、モデルは常に多数派のクラスを予測するだけで、見かけ上高い精度を達成できますが、これは少数派のクラスに対するモデルの性能を反映していません。" },
        { "text": "精度は、モデルのトレーニング時間の影響を受けるため", "isCorrect": false, "rationale": "精度は、モデルのトレーニング時間とは直接関係ありません。" },
        { "text": "精度は、モデルの汎化能力を測定しないため", "isCorrect": false, "rationale": "精度は、モデルの汎化能力を測定する一つの方法です。" }
      ]
    },
    {
      "question": "不均衡なデータセットの評価に適したメトリックはどれですか？",
      "hint": "これは、適合率と再現率のバランスを考慮したものです。",
      "options": [
        { "text": "精度（Accuracy）", "isCorrect": false, "rationale": "精度は不均衡なデータセットの評価には適していません。" },
        { "text": "F1スコア", "isCorrect": true, "rationale": "F1スコアは、適合率と再現率の調和平均であり、不均衡なデータセットにおいて、モデルの性能をより公平に評価できます。" },
        { "text": "平均絶対誤差（MAE）", "isCorrect": false, "rationale": "平均絶対誤差は、回帰タスクの評価に使用されます。" },
        { "text": "平均二乗誤差（MSE）", "isCorrect": false, "rationale": "平均二乗誤差は、回帰タスクの評価に使用されます。" }
      ]
    },
    {
      "question": "モデルの重みを初期化する際、なぜゼロで初期化してはいけないのですか？",
      "hint": "これは、学習プロセスにおける対称性の問題に関係します。",
      "options": [
        { "text": "勾配が非常に大きくなり、勾配爆発を引き起こすため", "isCorrect": false, "rationale": "勾配爆発は、重みが大きい場合に発生します。" },
        { "text": "すべてのニューロンが同じように学習し、モデルが特徴を学習できなくなるため", "isCorrect": true, "rationale": "すべての重みをゼロに初期化すると、すべてのニューロンが同じ入力を受け取り、同じ勾配を計算するため、対称性が崩れず、異なる特徴を学習できません。" },
        { "text": "モデルの学習速度が遅くなるため", "isCorrect": false, "rationale": "学習速度は重みの初期化とは直接関係ありません。" },
        { "text": "モデルが過学習する可能性が高くなるため", "isCorrect": false, "rationale": "過学習は、モデルの複雑さが原因で発生します。" }
      ]
    },
    {
      "question": "モデルの重みを初期化する際、なぜ小さなランダムな値で初期化するのですか？",
      "hint": "これは、対称性を破り、各ニューロンが異なる役割を果たすようにするためです。",
      "options": [
        { "text": "学習速度を上げるため", "isCorrect": false, "rationale": "学習速度は学習率によって制御されます。" },
        { "text": "モデルの汎化能力を向上させるため", "isCorrect": false, "rationale": "初期化は、汎化能力に影響を与えますが、主な目的ではありません。" },
        { "text": "モデルの過学習を防ぐため", "isCorrect": false, "rationale": "過学習を防ぐ主な目的は、正則化やドロップアウトです。" },
        { "text": "学習プロセスにおける対称性を破り、各ニューロンが異なる特徴を学習できるようにするため", "isCorrect": true, "rationale": "小さなランダムな値で重みを初期化することで、各ニューロンが異なる入力を受け取り、異なる勾配を計算するようになり、学習プロセスが円滑に進みます。" }
      ]
    },
    {
      "question": "勾配消失（Vanishing Gradient）の問題は、どのタイプのネットワークで特に顕著ですか？",
      "hint": "これは、深層学習モデルの初期の問題の一つです。",
      "options": [
        { "text": "非常に浅いニューラルネットワーク", "isCorrect": false, "rationale": "勾配消失は、深いネットワークでより顕著です。" },
        { "text": "ロジスティック回帰モデル", "isCorrect": false, "rationale": "ロジスティック回帰モデルは、ニューラルネットワークではありません。" },
        { "text": "非常に深いニューラルネットワーク", "isCorrect": true, "rationale": "勾配消失は、ネットワークが深くなるにつれて勾配が非常に小さくなり、最初の層の学習がほとんど進まなくなる現象です。" },
        { "text": "線形回帰モデル", "isCorrect": false, "rationale": "線形回帰モデルは、ニューラルネットワークではありません。" }
      ]
    },
    {
      "question": "勾配消失の問題を解決するための一般的な手法はどれですか？",
      "hint": "これは、学習率を調整するのではなく、勾配の大きさを管理するものです。",
      "options": [
        { "text": "勾配クリッピング（Gradient Clipping）", "isCorrect": false, "rationale": "勾配クリッピングは、勾配爆発を防ぐための手法です。" },
        { "text": "バッチ正規化（Batch Normalization）", "isCorrect": true, "rationale": "バッチ正規化は、層への入力の分布を安定させることで、勾配消失や勾配爆発の問題を緩和します。" },
        { "text": "小さな学習率を使用する", "isCorrect": false, "rationale": "小さな学習率を使用すると、勾配消失の問題が悪化する可能性があります。" },
        { "text": "ドロップアウト（Dropout）", "isCorrect": false, "rationale": "ドロップアウトは過学習を防ぐための手法です。" }
      ]
    },
    {
      "question": "勾配爆発（Exploding Gradient）の問題を解決するための一般的な手法はどれですか？",
      "hint": "これは、学習プロセスを安定させるためのものです。",
      "options": [
        { "text": "勾配クリッピング（Gradient Clipping）", "isCorrect": true, "rationale": "勾配クリッピングは、勾配が特定の閾値を超えないようにすることで、勾配爆発を防ぎ、学習の不安定性を減らします。" },
        { "text": "学習率を下げる", "isCorrect": true, "rationale": "学習率を下げることも、勾配爆発を緩和するのに役立ちます。" },
        { "text": "バッチ正規化（Batch Normalization）", "isCorrect": true, "rationale": "バッチ正規化は、勾配の爆発を防ぐのに役立ちます。" },
        { "text": "すべての重みをゼロに初期化する", "isCorrect": false, "rationale": "これは学習を妨げます。" }
      ]
    },
    {
      "question": "教師あり学習（Supervised Learning）と教師なし学習（Unsupervised Learning）の違いは何ですか？",
      "hint": "これは、トレーニングデータに『正解』があるかどうかの違いです。",
      "options": [
        { "text": "教師あり学習は、トレーニングデータにラベル（正解）が含まれているのに対し、教師なし学習は含まれていない", "isCorrect": true, "rationale": "教師あり学習は、ラベル付きデータを使用してモデルをトレーニングし、教師なし学習は、ラベルなしデータを使用してモデルをトレーニングします。" },
        { "text": "教師あり学習は、回帰タスクに使用され、教師なし学習は、分類タスクに使用される", "isCorrect": false, "rationale": "教師あり学習は、分類と回帰の両方に使用されます。" },
        { "text": "教師あり学習は、ニューラルネットワークを使用し、教師なし学習は、ニューラルネットワークを使用しない", "isCorrect": false, "rationale": "どちらの種類の学習にもニューラルネットワークを使用できます。" },
        { "text": "教師あり学習は、予測に使用され、教師なし学習は、特徴抽出に使用される", "isCorrect": false, "rationale": "どちらの種類の学習も、予測や特徴抽出に使用できます。" }
      ]
    },
    {
      "question": "回帰（Regression）タスクと分類（Classification）タスクの違いは何ですか？",
      "hint": "これは、予測される出力の性質の違いです。",
      "options": [
        { "text": "回帰は連続的な値を予測し、分類は離散的なクラスを予測する", "isCorrect": true, "rationale": "回帰は、株価や気温などの連続的な値を予測し、分類は、スパムメールか否かなどの離散的なクラスを予測します。" },
        { "text": "回帰は教師あり学習であり、分類は教師なし学習である", "isCorrect": false, "rationale": "どちらも教師あり学習です。" },
        { "text": "回帰は精度（Accuracy）で評価され、分類は平均絶対誤差（MAE）で評価される", "isCorrect": false, "rationale": "回帰は、平均絶対誤差（MAE）や平均二乗誤差（MSE）で評価され、分類は、精度（Accuracy）やF1スコアで評価されます。" },
        { "text": "回帰はニューラルネットワークを使用し、分類は決定木を使用する", "isCorrect": false, "rationale": "どちらの種類のタスクにもニューラルネットワークや決定木を使用できます。" }
      ]
    },
    {
      "question": "モデルのハイパーパラメータチューニングのプロセスで、トレーニングデータ、検証データ、テストデータに分割する理由は何ですか？",
      "hint": "これは、モデルの性能を公平に評価するためです。",
      "options": [
        { "text": "モデルのトレーニングを高速化するため", "isCorrect": false, "rationale": "データ分割は、モデルのトレーニング時間を短縮するものではありません。" },
        { "text": "トレーニング中に過学習を防ぐため", "isCorrect": false, "rationale": "早期停止は、過学習を防ぐための手法です。" },
        { "text": "モデルのハイパーパラメータを調整し、最終的な性能を公平に評価するため", "isCorrect": true, "rationale": "検証データはハイパーパラメータの調整に使用され、テストデータはモデルの最終的な性能を公平に評価するために使用されます。これにより、モデルが新しいデータでどれだけうまく機能するかをより正確に測定できます。" },
        { "text": "モデルの重みを初期化するため", "isCorrect": false, "rationale": "これは初期化の役割です。" }
      ]
    },
    {
      "question": "モデルのパフォーマンスを評価する際に使用されるROC曲線は何を測定しますか？",
      "hint": "これは、真陽性率と偽陽性率の関係を示すものです。",
      "options": [
        { "text": "モデルの精度（Accuracy）", "isCorrect": false, "rationale": "精度は、ROC曲線とは異なるメトリックです。" },
        { "text": "モデルがどれだけうまくクラスを分離できるか", "isCorrect": true, "rationale": "ROC曲線は、さまざまな閾値設定でのモデルの分類性能を示し、真陽性率（感度）と偽陽性率の関係をプロットします。曲線の下の面積（AUC）は、モデルがクラスをどれだけうまく分離できるかを示します。" },
        { "text": "モデルのトレーニング時間", "isCorrect": false, "rationale": "ROC曲線は、トレーニング時間とは関係ありません。" },
        { "text": "モデルの重みの分布", "isCorrect": false, "rationale": "ROC曲線は、重みの分布とは関係ありません。" }
      ]
    },
    {
      "question": "AUC（Area Under the Curve）メトリックは、何を表していますか？",
      "hint": "これは、ROC曲線の性能を単一の数値で要約するものです。",
      "options": [
        { "text": "モデルの学習率", "isCorrect": false, "rationale": "AUCは、学習率とは関係ありません。" },
        { "text": "モデルの汎化能力の全体的な尺度", "isCorrect": true, "rationale": "AUCは、モデルが陽性クラスと陰性クラスをどれだけうまく区別できるかの全体的な尺度を提供します。" },
        { "text": "モデルの重みの平均値", "isCorrect": false, "rationale": "AUCは、重みの平均値とは関係ありません。" },
        { "text": "モデルの過学習の程度", "isCorrect": false, "rationale": "AUCは、モデルの性能を評価するために使用されますが、過学習の程度を直接測定するものではありません。" }
      ]
    },
    {
      "question": "モデルの重みを初期化する際、He初期化がReLU活性化関数と相性が良い理由は何ですか？",
      "hint": "これは、ReLUの特性と勾配の伝播に関係します。",
      "options": [
        { "text": "勾配をゼロに設定するため", "isCorrect": false, "rationale": "勾配をゼロに設定することは、学習を停止させます。" },
        { "text": "学習速度を上げるため", "isCorrect": false, "rationale": "He初期化は、学習速度を直接的に制御するものではありません。" },
        { "text": "勾配消失の問題を緩和するため", "isCorrect": true, "rationale": "ReLUは、負の入力で勾配がゼロになるため、勾配消失の問題を引き起こす可能性があります。He初期化は、重みの分散を調整することで、勾配が消失するのを防ぎます。" },
        { "text": "モデルの過学習を防ぐため", "isCorrect": false, "rationale": "He初期化は、過学習を防ぐためのものではありません。" }
      ]
    },
    {
      "question": "モデルの量子化（Quantization）の目的は何ですか？",
      "hint": "これは、モデルを『軽量化』するための技術です。",
      "options": [
        { "text": "モデルの精度を向上させること", "isCorrect": false, "rationale": "モデルの量子化は、精度を犠牲にしてモデルのサイズを減らします。" },
        { "text": "モデルのサイズを減らし、推論速度を上げるために、重みを低精度で表現すること", "isCorrect": true, "rationale": "モデルの量子化は、モデルの重みを浮動小数点数から整数に変換することで、モデルのサイズを減らし、推論速度を上げます。" },
        { "text": "モデルのトレーニング時間を短縮すること", "isCorrect": false, "rationale": "モデルの量子化は、推論速度を上げるためのものです。" },
        { "text": "モデルの汎化能力を向上させること", "isCorrect": false, "rationale": "モデルの量子化は、汎化能力に影響を与えますが、主な目的ではありません。" }
      ]
    },
    {
      "question": "モデルの蒸留（Distillation）の目的は何ですか？",
      "hint": "これは、より大きなモデルの知識を小さなモデルに『転送』するものです。",
      "options": [
        { "text": "モデルのサイズを増やすこと", "isCorrect": false, "rationale": "モデルの蒸留は、モデルのサイズを減らすことを目的としています。" },
        { "text": "モデルの精度を向上させること", "isCorrect": false, "rationale": "モデルの蒸留は、精度を犠牲にしてモデルのサイズを減らします。" },
        { "text": "より大きなモデルの知識を、より小さなモデルに転送すること", "isCorrect": true, "rationale": "モデルの蒸留は、より大きな教師モデルのソフトラベルを使用して、より小さな生徒モデルをトレーニングすることで、モデルのサイズを減らし、高速化します。" },
        { "text": "モデルのトレーニング時間を短縮すること", "isCorrect": false, "rationale": "モデルの蒸留は、推論速度を上げるためのものです。" }
      ]
    },
    {
      "question": "モデルのプルーニング（Pruning）の目的は何ですか？",
      "hint": "これは、モデルの『枝刈り』を行うものです。",
      "options": [
        { "text": "モデルの精度を向上させること", "isCorrect": false, "rationale": "モデルのプルーニングは、精度を犠牲にしてモデルのサイズを減らします。" },
        { "text": "モデルのトレーニング時間を短縮すること", "isCorrect": false, "rationale": "モデルのプルーニングは、推論速度を上げるためのものです。" },
        { "text": "モデルの不要な接続やニューロンを削除し、サイズと推論速度を向上させること", "isCorrect": true, "rationale": "モデルのプルーニングは、モデルのサイズを減らし、推論速度を上げるために、重要でない接続やニューロンを削除します。" },
        { "text": "モデルの汎化能力を向上させること", "isCorrect": false, "rationale": "モデルのプルーニングは、汎化能力に影響を与えますが、主な目的ではありません。" }
      ]
    },
    {
      "question": "モデルの重み共有（Weight Sharing）は、どのタイプのネットワークでよく使用されますか？",
      "hint": "これは、畳み込みニューラルネットワーク（CNN）の主要な特徴の一つです。",
      "options": [
        { "text": "リカレントニューラルネットワーク（RNN）", "isCorrect": false, "rationale": "RNNは、時系列データに対して重みを共有します。" },
        { "text": "畳み込みニューラルネットワーク（CNN）", "isCorrect": true, "rationale": "畳み込みニューラルネットワークは、重み共有を使用して、画像の特徴を効率的に学習します。" },
        { "text": "パーセプトロン", "isCorrect": false, "rationale": "パーセプトロンは、重み共有を使用しません。" },
        { "text": "線形回帰モデル", "isCorrect": false, "rationale": "線形回帰モデルは、重み共有を使用しません。" }
      ]
    },
    {
      "question": "データ増強（Data Augmentation）の目的は何ですか？",
      "hint": "これは、モデルが新しいデータでうまく機能するように、データの多様性を増やすためのものです。",
      "options": [
        { "text": "トレーニング時間を短縮すること", "isCorrect": false, "rationale": "データ増強は、トレーニング時間を延長します。" },
        { "text": "モデルのサイズを減らすこと", "isCorrect": false, "rationale": "データ増強は、モデルのサイズとは関係ありません。" },
        { "text": "既存のデータを変換して、トレーニングデータのサイズと多様性を増やすこと", "isCorrect": true, "rationale": "データ増強は、既存のデータに変換（回転、反転、拡大縮小など）を適用することで、トレーニングデータのサイズと多様性を増やし、モデルの汎化能力を向上させます。" },
        { "text": "モデルの重みを初期化すること", "isCorrect": false, "rationale": "これは初期化の役割です。" }
      ]
    },
    {
      "question": "モデルのアンサンブル（Ensemble）の目的は何ですか？",
      "hint": "これは、複数のモデルの予測を組み合わせることで、より良い結果を得るものです。",
      "options": [
        { "text": "モデルのサイズを減らすこと", "isCorrect": false, "rationale": "モデルのアンサンブルは、モデルのサイズを減らすものではありません。" },
        { "text": "モデルの予測を組み合わせて、よりロバストで正確な予測を得ること", "isCorrect": true, "rationale": "モデルのアンサンブルは、複数のモデルの予測を組み合わせることで、個々のモデルの予測よりも優れた結果を達成します。" },
        { "text": "モデルのトレーニング時間を短縮すること", "isCorrect": false, "rationale": "モデルのアンサンブルは、トレーニング時間を延長します。" },
        { "text": "モデルの重みをゼロに初期化すること", "isCorrect": false, "rationale": "これは初期化の役割です。" }
      ]
    },
    {
      "question": "モデルのバギング（Bagging）とは何ですか？",
      "hint": "これは、複数のモデルをトレーニングし、その予測を平均化または投票で決定するものです。",
      "options": [
        { "text": "複数のモデルを順番にトレーニングすること", "isCorrect": false, "rationale": "これはブースティング（Boosting）です。" },
        { "text": "同じアルゴリズムで、異なるトレーニングデータのサブセットで複数のモデルをトレーニングすること", "isCorrect": true, "rationale": "バギングは、元のデータセットからランダムにサンプリングされたサブセットで複数のモデルを独立してトレーニングし、その予測を組み合わせることで、過学習を減らし、性能を向上させます。" },
        { "text": "異なるアルゴリズムで、同じデータで複数のモデルをトレーニングすること", "isCorrect": false, "rationale": "これはスタッキング（Stacking）です。" },
        { "text": "単一のモデルをトレーニングすること", "isCorrect": false, "rationale": "これは単一モデルのトレーニングです。" }
      ]
    },
    {
      "question": "モデルのブースティング（Boosting）とは何ですか？",
      "hint": "これは、弱い学習器を順番にトレーニングし、前のモデルの誤りを修正するものです。",
      "options": [
        { "text": "弱い学習器を順番にトレーニングし、前のモデルの誤りを修正すること", "isCorrect": true, "rationale": "ブースティングは、弱い学習器を順番に構築し、各モデルが前のモデルの誤りを修正するように学習します。" },
        { "text": "同じアルゴリズムで、異なるトレーニングデータのサブセットで複数のモデルをトレーニングすること", "isCorrect": false, "rationale": "これはバギング（Bagging）です。" },
        { "text": "異なるアルゴリズムで、同じデータで複数のモデルをトレーニングすること", "isCorrect": false, "rationale": "これはスタッキング（Stacking）です。" },
        { "text": "単一のモデルをトレーニングすること", "isCorrect": false, "rationale": "これは単一モデルのトレーニングです。" }
      ]
    },
    {
      "question": "モデルのスタッキング（Stacking）とは何ですか？",
      "hint": "これは、複数のモデルの予測を、別のモデルで組み合わせて最終的な予測を行うものです。",
      "options": [
        { "text": "複数のモデルを順番にトレーニングすること", "isCorrect": false, "rationale": "これはブースティング（Boosting）です。" },
        { "text": "同じアルゴリズムで、異なるトレーニングデータのサブセットで複数のモデルをトレーニングすること", "isCorrect": false, "rationale": "これはバギング（Bagging）です。" },
        { "text": "複数のモデルの予測を、別のモデルで組み合わせて最終的な予測を行うこと", "isCorrect": true, "rationale": "スタッキングは、複数の基本モデルの予測を、メタモデルと呼ばれる別のモデルの入力として使用し、最終的な予測を行います。" },
        { "text": "単一のモデルをトレーニングすること", "isCorrect": false, "rationale": "これは単一モデルのトレーニングです。" }
      ]
    },
    {
      "question": "ハイパーパラメータチューニングのプロセスで、グリッドサーチを使用する際の主な欠点は何ですか？",
      "hint": "これは、計算リソースの消費量と関係があります。",
      "options": [
        { "text": "探索空間が広すぎる場合、計算コストが非常に高くなること", "isCorrect": true, "rationale": "グリッドサーチは、すべてのパラメータの組み合わせを試すため、探索空間が広すぎると計算コストが非常に高くなります。" },
        { "text": "局所最適解に陥る可能性が高いこと", "isCorrect": false, "rationale": "グリッドサーチは、局所最適解に陥る可能性は低いですが、効率的ではありません。" },
        { "text": "手動でパラメータを設定する必要があること", "isCorrect": false, "rationale": "グリッドサーチは、パラメータの値を自動的に試行します。" },
        { "text": "モデルの汎化能力を向上させることができないこと", "isCorrect": false, "rationale": "グリッドサーチは、モデルの汎化能力を向上させるための手法です。" }
      ]
    },
    {
      "question": "ハイパーパラメータチューニングのプロセスで、ランダムサーチを使用する際の主な利点は何ですか？",
      "hint": "これは、グリッドサーチと比較して、効率が良いことです。",
      "options": [
        { "text": "探索空間が広い場合でも、効率的に最適な組み合わせを見つけることができること", "isCorrect": true, "rationale": "ランダムサーチは、グリッドサーチよりも効率的に最適な組み合わせを見つけることができます。" },
        { "text": "すべてのパラメータの組み合わせを試すことができること", "isCorrect": false, "rationale": "これはグリッドサーチの利点です。" },
        { "text": "手動でパラメータを設定する必要がないこと", "isCorrect": false, "rationale": "ランダムサーチは、パラメータの値を自動的に試行します。" },
        { "text": "モデルの過学習を防ぐことができること", "isCorrect": false, "rationale": "ランダムサーチは、過学習を防ぐための手法ではありません。" }
      ]
    },
    {
      "question": "モデルの性能を評価する際、適合率（Precision）とは何を測定しますか？",
      "hint": "これは、モデルが陽性と予測したサンプルの中で、実際に陽性であった割合を測定するものです。",
      "options": [
        { "text": "真陽性の数 / (真陽性の数 + 偽陽性の数)", "isCorrect": true, "rationale": "適合率は、モデルが正と予測したサンプルの中で、実際に正であった割合を測定します。" },
        { "text": "真陽性の数 / (真陽性の数 + 偽陰性の数)", "isCorrect": false, "rationale": "これは再現率（Recall）です。" },
        { "text": "(真陽性の数 + 真陰性の数) / 全体の数", "isCorrect": false, "rationale": "これは精度（Accuracy）です。" },
        { "text": "真陽性の数 + 偽陽性の数", "isCorrect": false, "rationale": "これは適合率ではありません。" }
      ]
    },
    {
      "question": "モデルの性能を評価する際、再現率（Recall）とは何を測定しますか？",
      "hint": "これは、すべての陽性サンプルの中で、モデルが正しく予測した割合を測定するものです。",
      "options": [
        { "text": "真陽性の数 / (真陽性の数 + 偽陽性の数)", "isCorrect": false, "rationale": "これは適合率（Precision）です。" },
        { "text": "真陽性の数 / (真陽性の数 + 偽陰性の数)", "isCorrect": true, "rationale": "再現率は、すべての正のサンプルの中で、モデルが正しく予測した割合を測定します。" },
        { "text": "(真陽性の数 + 真陰性の数) / 全体の数", "isCorrect": false, "rationale": "これは精度（Accuracy）です。" },
        { "text": "真陽性の数 + 偽陰性の数", "isCorrect": false, "rationale": "これは再現率ではありません。" }
      ]
    },
    {
      "question": "F1スコアの目的は何ですか？",
      "hint": "これは、適合率と再現率のバランスを評価するためのものです。",
      "options": [
        { "text": "モデルのトレーニング時間を評価すること", "isCorrect": false, "rationale": "F1スコアは、モデルの性能を評価するためのメトリックです。" },
        { "text": "適合率と再現率の間のトレードオフをバランスさせること", "isCorrect": true, "rationale": "F1スコアは、適合率と再現率の調和平均であり、両方のメトリックを考慮してモデルの性能を評価します。" },
        { "text": "モデルの過学習を検出すること", "isCorrect": false, "rationale": "F1スコアは、過学習を直接検出するものではありません。" },
        { "text": "モデルの重みを初期化すること", "isCorrect": false, "rationale": "これは初期化の役割です。" }
      ]
    },
    {
      "question": "モデルの性能を評価する際、混同行列（Confusion Matrix）は何を示しますか？",
      "hint": "これは、モデルの予測と実際のクラスの関係を示すものです。",
      "options": [
        { "text": "モデルのトレーニング時間", "isCorrect": false, "rationale": "混同行列は、トレーニング時間とは関係ありません。" },
        { "text": "モデルの重みの分布", "isCorrect": false, "rationale": "混同行列は、重みの分布とは関係ありません。" },
        { "text": "モデルの予測と実際のクラスの関係", "isCorrect": true, "rationale": "混同行列は、真陽性、偽陽性、偽陰性、真陰性の数を表形式で示し、モデルの性能を詳細に分析するのに役立ちます。" },
        { "text": "モデルの汎化能力の全体的な尺度", "isCorrect": false, "rationale": "混同行列は、モデルの性能を詳細に分析するのに役立ちますが、全体的な尺度ではありません。" }
      ]
    },
    {
      "question": "トレーニングデータが少ない場合、モデルの性能を向上させるための最も効果的な手法はどれですか？",
      "hint": "これは、既存のデータをより多く活用するものです。",
      "options": [
        { "text": "モデルのサイズを大きくする", "isCorrect": false, "rationale": "モデルのサイズを大きくすると、過学習する可能性が高くなります。" },
        { "text": "学習率を下げる", "isCorrect": false, "rationale": "学習率を下げることは、トレーニングを遅くする可能性があります。" },
        { "text": "データ増強（Data Augmentation）", "isCorrect": true, "rationale": "データ増強は、既存のデータを変換して、トレーニングデータのサイズと多様性を増やし、モデルの汎化能力を向上させます。" },
        { "text": "早期停止（Early Stopping）", "isCorrect": false, "rationale": "早期停止は、過学習を防ぐための手法です。" }
      ]
    },
    {
      "question": "モデルの性能がテストデータで低下する原因として最も可能性が高いのはどれですか？",
      "hint": "これは、モデルがトレーニングデータに特化しすぎている状態です。",
      "options": [
        { "text": "過学習（Overfitting）", "isCorrect": true, "rationale": "過学習は、モデルがトレーニングデータ内のノイズや特定のパターンを過剰に学習した結果、未知のデータに対する汎化能力が失われる現象です。" },
        { "text": "過小学習（Underfitting）", "isCorrect": false, "rationale": "過小学習は、モデルがトレーニングデータでもうまく機能しないことです。" },
        { "text": "適切な学習率の使用", "isCorrect": false, "rationale": "適切な学習率は、モデルの性能を向上させます。" },
        { "text": "早期停止（Early Stopping）", "isCorrect": false, "rationale": "早期停止は、過学習を防ぐための手法です。" }
      ]
    },
    {
      "question": "ハイパーパラメータチューニングのプロセスで、グリッドサーチとランダムサーチを組み合わせることはできますか？",
      "hint": "これは、両方の手法の利点を活用するものです。",
      "options": [
        { "text": "はい、できません", "isCorrect": false, "rationale": "グリッドサーチとランダムサーチを組み合わせることは可能です。" },
        { "text": "はい、できます。これは、ハイブリッドサーチとして知られています。", "isCorrect": true, "rationale": "ハイブリッドサーチは、グリッドサーチとランダムサーチを組み合わせることで、探索空間をより効率的に探索します。" },
        { "text": "いいえ、できません。これらは相反する手法です。", "isCorrect": false, "rationale": "グリッドサーチとランダムサーチを組み合わせることは可能です。" },
        { "text": "はい、できます。これは、ベイズ最適化として知られています。", "isCorrect": false, "rationale": "ベイズ最適化は、過去の結果に基づいて次の試行点をインテリジェントに選択する、より高度な手法です。" }
      ]
    },
    {
      "question": "モデルの精度がテストデータで90%の場合、これは何を意味しますか？",
      "hint": "これは、モデルが正しく予測したサンプルの割合です。",
      "options": [
        { "text": "モデルがトレーニングデータに過学習していること", "isCorrect": false, "rationale": "モデルの精度がテストデータで高い場合、過学習している可能性は低いです。" },
        { "text": "モデルが90%の確率で正しく予測すること", "isCorrect": true, "rationale": "精度が90%の場合、モデルがテストデータの90%を正しく予測したことを意味します。" },
        { "text": "モデルが10%の確率で正しく予測すること", "isCorrect": false, "rationale": "これは間違いです。" },
        { "text": "モデルが過小学習していること", "isCorrect": false, "rationale": "モデルの精度が高い場合、過小学習している可能性は低いです。" }
      ]
    },
    {
      "question": "モデルの学習率が大きすぎる場合、損失関数はどのように動作しますか？",
      "hint": "これは、損失が収束せずに変動することに関係します。",
      "options": [
        { "text": "損失が安定して減少し、最小値に収束する", "isCorrect": false, "rationale": "これは、学習率が適切な場合に発生します。" },
        { "text": "損失が収束せず、振動するまたは発散する", "isCorrect": true, "rationale": "学習率が大きすぎる場合、モデルが最適解を『飛び越えて』しまい、損失が収束せずに発散したり、不安定になったりする可能性があります。" },
        { "text": "損失が非常にゆっくりと減少する", "isCorrect": false, "rationale": "これは、学習率が小さすぎる場合に発生します。" },
        { "text": "損失がゼロになる", "isCorrect": false, "rationale": "損失がゼロになることは、過学習を示します。" }
      ]
    },
    {
      "question": "モデルの学習率が小さすぎる場合、損失関数はどのように動作しますか？",
      "hint": "これは、損失が非常にゆっくりと減少することに関係します。",
      "options": [
        { "text": "損失が安定して減少し、最小値に収束する", "isCorrect": false, "rationale": "これは、学習率が適切な場合に発生します。" },
        { "text": "損失が収束せず、振動するまたは発散する", "isCorrect": false, "rationale": "これは、学習率が大きすぎる場合に発生します。" },
        { "text": "損失が非常にゆっくりと減少し、最適解に到達するのに時間がかかる", "isCorrect": true, "rationale": "学習率が小さすぎる場合、モデルのパラメータ更新が非常に小さくなり、モデルの収束が遅くなり、最適解に到達するまでに多くの反復が必要になります。" },
        { "text": "損失がゼロになる", "isCorrect": false, "rationale": "損失がゼロになることは、過学習を示します。" }
      ]
    }
];

let currentQuestionIndex = 0;
const quizContent = document.getElementById('quiz-content');

function renderQuestion() {
    const q = questions[currentQuestionIndex];
    
    // Shuffle the options
    const shuffledOptions = [...q.options].sort(() => Math.random() - 0.5);

    let optionsHtml = '';
    shuffledOptions.forEach((option, index) => {
        optionsHtml += `
            <button class="answer-button" data-index="${index}">${option.text}</button>
        `;
    });

    const questionHtml = `
        <div class="question-card">
            <div class="flex justify-between items-center mb-4">
                <span class="text-lg font-bold text-gray-700">問題 ${currentQuestionIndex + 1}/${questions.length}</span>
                <span class="text-sm text-gray-500 italic">ヒント: ${q.hint}</span>
            </div>
            <p class="question-text">${q.question}</p>
            <div id="options-container">
                ${optionsHtml}
            </div>
            <div id="feedback-container" class="hidden mt-4"></div>
        </div>
        <div class="quiz-footer">
            <span id="progress-text" class="progress-text">進捗: ${currentQuestionIndex + 1} / ${questions.length}</span>
            <button id="next-button" class="next-button hidden">次へ</button>
        </div>
    `;

    quizContent.innerHTML = questionHtml;
    attachEventListeners(q, shuffledOptions);
}

function attachEventListeners(q, shuffledOptions) {
    const optionsContainer = document.getElementById('options-container');
    const feedbackContainer = document.getElementById('feedback-container');
    const nextButton = document.getElementById('next-button');

    optionsContainer.addEventListener('click', (e) => {
        if (e.target.classList.contains('answer-button')) {
            const selectedIndex = parseInt(e.target.dataset.index);
            const selectedOption = shuffledOptions[selectedIndex];
            
            // Disable all buttons after an answer is selected
            Array.from(optionsContainer.children).forEach(button => {
                button.disabled = true;
                const buttonIndex = parseInt(button.dataset.index);
                const option = shuffledOptions[buttonIndex];
                if (option.isCorrect) {
                    button.classList.add('correct');
                }
            });

            if (!selectedOption.isCorrect) {
                e.target.classList.add('incorrect');
            }

            // Show explanation
            feedbackContainer.classList.remove('hidden');
            feedbackContainer.innerHTML = `
                <div class="explanation">
                    <p class="explanation-text font-semibold mb-2">${selectedOption.isCorrect ? '正解です！' : '不正解です...'}</p>
                    <p class="explanation-text">${selectedOption.rationale}</p>
                </div>
            `;
            
            // Show next button
            nextButton.classList.remove('hidden');
        }
    });

    nextButton.addEventListener('click', () => {
        currentQuestionIndex++;
        if (currentQuestionIndex < questions.length) {
            renderQuestion();
        } else {
            quizContent.innerHTML = `
                <h2 class="text-2xl font-bold text-center text-gray-800 mb-4">クイズ完了！</h2>
                <p class="text-center text-gray-600">お疲れ様でした！これでAIチューニングの基本をマスターできましたね。</p>
            `;
            document.querySelector('.quiz-footer').style.display = 'none';
        }
    });
}

// Initialize the quiz
window.onload = renderQuestion;
</script>

</body>
</html>
